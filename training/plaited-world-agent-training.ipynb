{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Plaited World Agent Training\n\nTrain a UI generation agent using Unsloth on Google Colab.\n\n## Training Pipeline Overview\n\n```mermaid\nflowchart TB\n    subgraph Training[\"Training Pipeline\"]\n        Notebook[\"training/plaited-world-agent-training.ipynb\"]\n        SFT[\"1. SFT<br/>(Developer writes)\"]\n        DPO[\"2. DPO<br/>(Developer preferences)\"]\n    end\n\n    subgraph Local[\"Local Training (Requires Browser)\"]\n        GRPO[\"3. GRPO<br/>(Model generates ‚Üí Browser validates)\"]\n    end\n\n    Notebook --> SFT --> DPO\n    DPO -->|\"Trained Model\"| GRPO\n```\n\n**Note:** GRPO requires browser execution for reward computation and runs locally with `plaited-world-agent-grpo.ipynb`.\n\n## Training Phases\n1. **SFT (Supervised Fine-Tuning)** - Learn from gold examples (this notebook)\n2. **DPO (Direct Preference Optimization)** - Learn from preference pairs (optional, this notebook)\n3. **GRPO (Group Relative Policy Optimization)** - Model generates, browser validates (separate notebook, requires local setup)\n\n## Trajectory Format\n\nThe training data now includes tiered analysis results and structural metadata:\n\n```json\n{\n  \"messages\": [...],\n  \"reward\": 0.85,\n  \"tiers\": {\n    \"static\": {\"passed\": true, \"tier\": 1, \"checks\": [...]},\n    \"browser\": {\"passed\": true, \"a11yPassed\": true, \"totalAssertions\": 5, \"passedAssertions\": 5}\n  },\n  \"structural\": {\n    \"objects\": [{\"name\": \"Button\", \"grouping\": \"nested\"}],\n    \"channel\": \"selection\",\n    \"loops\": [{\"trigger\": \"click\", \"handler\": \"click\"}],\n    \"levers\": [\"button\"],\n    \"block\": \"card\"\n  }\n}\n```\n\n## Prerequisites\n\n### 1. Enable GPU Runtime\nRuntime ‚Üí Change runtime type ‚Üí **T4 GPU**\n\n### 2. Upload Training Files\nClick üìÅ in left sidebar, then upload:\n- `trajectories.jsonl` (required) - Generate with: `bun scripts/generate-trajectories.ts training/stories -o training/trajectories.jsonl`\n- `dpo-preferences.jsonl` (optional, for DPO phase)\n\n### 3. Add Colab Secrets\nClick üîë in left sidebar, add these secrets:\n\n| Secret Name | Value | Example |\n|-------------|-------|---------|\n| `HF_TOKEN` | Your HuggingFace token (with write access) | `hf_xxxxx` |\n| `HF_USERNAME` | HuggingFace org or username | `plaited` |\n| `HF_MODEL_NAME` | Model name to push | `plaited-world-agent-lora` |\n\nToggle \"Notebook access\" ON for each secret.",
   "metadata": {
    "id": "intro"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": "# Cell 1: Install Dependencies (pinned versions for Unsloth compatibility)\n!pip install unsloth\n!pip install trl==0.24.0 datasets==4.3.0 transformers"
  },
  {
   "cell_type": "code",
   "source": "# Cell 2: Load Secrets and Login to HuggingFace\nfrom huggingface_hub import login\nfrom google.colab import userdata\n\n# Load secrets from Colab (set via üîë sidebar)\nhf_token = userdata.get('HF_TOKEN')\nhf_username = userdata.get('HF_USERNAME')\nhf_model_name = userdata.get('HF_MODEL_NAME')\n\n# Login to HuggingFace\nlogin(token=hf_token)\n\nprint(f\"‚úì Logged in to HuggingFace\")\nprint(f\"‚úì Will push to: {hf_username}/{hf_model_name}\")",
   "metadata": {
    "id": "login"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 3: Load Model with Unsloth\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"google/functiongemma-270m-it\",  # Function calling optimized (270M params)\n    max_seq_length=2048,\n    dtype=None,  # Auto-detect\n    load_in_4bit=True,  # 4-bit quantization for memory efficiency\n)\n\n# Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=42,\n)\n\nprint(f\"Model loaded with {model.num_parameters():,} parameters\")",
   "metadata": {
    "id": "load_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 4: Load Training Data\nfrom datasets import load_dataset\n\n# Upload your trajectories.jsonl to Colab or load from HuggingFace\ndataset = load_dataset(\"json\", data_files=\"trajectories.jsonl\", split=\"train\")\n\nprint(f\"Loaded {len(dataset)} trajectories\")\nprint(f\"Sample intent: {dataset[0]['messages'][1]['content']}\")\n\ndef format_for_training(example):\n    \"\"\"Format trajectory for GRPO training.\"\"\"\n    messages = example[\"messages\"]\n    reward = example[\"reward\"]\n\n    # Format as chat template\n    text = tokenizer.apply_chat_template(messages, tokenize=False)\n\n    return {\n        \"text\": text,\n        \"reward\": reward\n    }\n\ndataset = dataset.map(format_for_training)\nprint(\"Dataset formatted for training\")",
   "metadata": {
    "id": "load_data"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: Configure SFT Training (Phase 1)\nfrom trl import SFTConfig, SFTTrainer\n\nconfig = SFTConfig(\n    output_dir=\"./sft-output\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    warmup_ratio=0.1,\n    logging_steps=10,\n    save_steps=100,\n    fp16=True,\n    max_seq_length=2048,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=config,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n)\n\nprint(\"SFT Trainer configured\")",
   "metadata": {
    "id": "configure"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 6: Train\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ],
   "metadata": {
    "id": "train"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Phase 2: DPO Training (Optional)\n\nAfter SFT, you can further improve the model with Direct Preference Optimization (DPO). DPO requires pairs of (chosen, rejected) responses for the same intent.\n\n**When to use DPO:**\n- You have examples of good AND bad generations for the same intent\n- You want the model to learn subtle quality differences\n- SFT alone isn't producing high-quality outputs\n\n**Data format for DPO:**\n```json\n{\n  \"prompt\": \"Create a primary button with hover state\",\n  \"chosen\": \"<start_function_call>call:writeTemplate{...good output...}<end_function_call>\",\n  \"rejected\": \"<start_function_call>call:writeTemplate{...bad output...}<end_function_call>\"\n}\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 7: DPO Training (Phase 2 - Optional)\n# Skip this cell if you only want SFT training\n# Requires: dpo-preferences.jsonl with prompt/chosen/rejected fields\n\nfrom trl import DPOConfig, DPOTrainer\nfrom datasets import load_dataset\n\n# Load preference pairs (upload dpo-preferences.jsonl first)\ntry:\n    dpo_dataset = load_dataset(\"json\", data_files=\"dpo-preferences.jsonl\", split=\"train\")\n    \n    dpo_config = DPOConfig(\n        output_dir=\"./dpo-output\",\n        num_train_epochs=1,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=8,\n        learning_rate=5e-6,  # Lower LR for DPO\n        warmup_ratio=0.1,\n        logging_steps=10,\n        save_steps=50,\n        fp16=True,\n        max_length=2048,\n        max_prompt_length=512,\n        beta=0.1,  # KL penalty coefficient\n    )\n    \n    dpo_trainer = DPOTrainer(\n        model=model,\n        ref_model=None,  # Use implicit reference (recommended for LoRA)\n        args=dpo_config,\n        train_dataset=dpo_dataset,\n        tokenizer=tokenizer,\n    )\n    \n    print(\"Starting DPO training...\")\n    dpo_trainer.train()\n    print(\"DPO training complete!\")\n    \nexcept FileNotFoundError:\n    print(\"No dpo-preferences.jsonl found. Skipping DPO phase.\")\n    print(\"To use DPO, create preference pairs from model generations.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 8: Save and Push to Hub\nMODEL_NAME = f\"{hf_username}/{hf_model_name}\"\n\n# Save locally first\nmodel.save_pretrained(f\"./{hf_model_name}\")\ntokenizer.save_pretrained(f\"./{hf_model_name}\")\nprint(\"Saved locally\")\n\n# Push to HuggingFace Hub\nmodel.push_to_hub(MODEL_NAME, token=hf_token)\ntokenizer.push_to_hub(MODEL_NAME, token=hf_token)\n\nprint(f\"\\nModel pushed to: https://huggingface.co/{MODEL_NAME}\")\nprint(\"\\nNext steps:\")\nprint(\"1. Go to https://huggingface.co/endpoints\")\nprint(\"2. Create new endpoint with your model\")\nprint(\"3. Select T4 GPU and vLLM container\")\nprint(\"4. Copy endpoint URL for use in your agent\")",
   "metadata": {
    "id": "push"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory\n",
    "- Reduce `per_device_train_batch_size` to 2 or 1\n",
    "- Increase `gradient_accumulation_steps` to compensate\n",
    "\n",
    "### Model Not Generating Tools\n",
    "- Check tool schema format matches training data\n",
    "- Try increasing temperature to 0.7\n",
    "\n",
    "### push_to_hub Fails\n",
    "- Verify HF token has write access\n",
    "- Check Colab secrets are set correctly"
   ],
   "metadata": {
    "id": "troubleshooting"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Connecting the Trained Agent\n\nAfter deploying to HuggingFace Inference Endpoints, connect from TypeScript:\n\n```typescript\nimport { InferenceClient } from '@huggingface/inference'\nimport { useWorldAgent, createCoreTools } from 'plaited/agent'\n\nconst client = new InferenceClient(process.env.HF_TOKEN)\n\nconst trigger = await useWorldAgent({\n  tools: createCoreTools({ outputDir: './generated' }),\n  \n  // Optional: Discover and register skill scripts as callable tools\n  skills: {\n    skillsRoot: '.claude/skills',  // Scans for scripts/ directories\n    timeout: 30000,                 // Script execution timeout\n  },\n  \n  // Optional: Custom system prompt (skill context is auto-appended)\n  systemPrompt: 'You are a Plaited UI generation agent.',\n  \n  model: {\n    chatCompletion: async (args) => {\n      const response = await client.chatCompletion({\n        model: 'your-username/plaited-world-agent',\n        endpointUrl: 'https://xxx.endpoints.huggingface.cloud',\n        messages: args.messages,\n        tools: args.tools?.map(t => ({ type: 'function', function: t }))\n      })\n      return {\n        tool_calls: response.choices[0]?.message?.tool_calls?.map(tc => ({\n          name: tc.function.name,\n          arguments: tc.function.arguments\n        }))\n      }\n    }\n  }\n})\n\n// Generate UI from intent\ntrigger({\n  type: 'generate',\n  detail: { intent: 'Create a primary button with hover state' }\n})\n```",
   "metadata": {}
  }
 ]
}