# LLM Judge Templates

Evaluation prompt templates for scoring agent trajectories.

## Anti-Bias Guidelines

When evaluating agent output, consider these biases from research:

1. **Familiarity bias** - Judges favor "familiar" patterns over novel approaches
2. **Output-only evaluation** - Missing trajectory context encourages mode collapse
3. **Position bias** - Earlier examples may be scored differently than later ones

**Mitigations:**
- Evaluate trajectories, not just final output
- Use structured scoring criteria
- Randomize evaluation order for batched runs

## Template Categories

| Category | Use When |
|----------|----------|
| Binary correctness | Pass/fail determination |
| Tool appropriateness | Evaluating tool selection |
| Reasoning quality | Assessing thought process |
| Code quality | Reviewing generated code |
| Efficiency | Measuring trajectory length |

## Binary Correctness

Simple pass/fail evaluation:

```
You are evaluating an AI coding agent's performance on a task.

## Task
{input}

## Agent Output
{output}

## Expected Behavior
{expected}

## Instructions
Determine if the agent successfully completed the task.

Respond in JSON:
{
  "passed": true | false,
  "reasoning": "Brief explanation"
}
```

## Tool Appropriateness

Evaluate tool selection:

```
You are evaluating an AI agent's tool usage.

## Task
{input}

## Trajectory
{trajectory}

## Instructions
Evaluate whether the agent used appropriate tools for this task.

Consider:
1. Were the right tools selected for each step?
2. Were any unnecessary tools used?
3. Were tools used in an efficient order?

Score (1-3):
- 1: Wrong tools or unnecessary tool usage
- 2: Correct tools but suboptimal order or redundant calls
- 3: Optimal tool selection and ordering

Respond in JSON:
{
  "score": 1 | 2 | 3,
  "tool_issues": ["list any problematic tool calls"],
  "reasoning": "Explanation"
}
```

## Reasoning Quality

Assess the thought process:

```
You are evaluating an AI agent's reasoning process.

## Task
{input}

## Trajectory
{trajectory}

## Instructions
Evaluate the coherence and quality of the agent's reasoning.

Consider:
1. Is the reasoning logical and connected?
2. Do thoughts lead naturally to actions?
3. Does the agent recover gracefully from errors?
4. Is the plan followed or appropriately adjusted?

Score (1-3):
- 1: Incoherent reasoning or failed to recover from errors
- 2: Generally coherent but with gaps or redundant steps
- 3: Clear, logical reasoning with appropriate adaptations

Respond in JSON:
{
  "score": 1 | 2 | 3,
  "coherence_issues": ["list any reasoning gaps"],
  "reasoning": "Explanation"
}
```

## Code Quality

For evaluations that produce code:

```
You are evaluating code generated by an AI agent.

## Task
{input}

## Generated Code
{code_from_trajectory}

## Instructions
Evaluate the quality of the generated code.

Consider:
1. Does it solve the stated problem?
2. Is it well-structured and readable?
3. Does it follow common patterns and best practices?
4. Are there any bugs or edge cases missed?

Score (1-3):
- 1: Broken, buggy, or fundamentally wrong approach
- 2: Works but has style issues, minor bugs, or missing edge cases
- 3: Clean, correct, well-structured code

Respond in JSON:
{
  "score": 1 | 2 | 3,
  "issues": ["list specific code issues"],
  "reasoning": "Explanation"
}
```

## Efficiency Scoring

Measure trajectory efficiency:

```
You are evaluating an AI agent's efficiency.

## Task
{input}

## Trajectory Summary
- Total steps: {step_count}
- Tool calls: {tool_call_count}
- Duration: {duration}ms

## Trajectory Details
{trajectory}

## Instructions
Evaluate whether the agent completed the task efficiently.

Consider:
1. Could the task have been completed with fewer steps?
2. Were there redundant tool calls?
3. Was information gathered efficiently?

Score (1-3):
- 1: Very inefficient (2x+ expected steps)
- 2: Somewhat inefficient (some redundant steps)
- 3: Efficient (minimal necessary steps)

Respond in JSON:
{
  "score": 1 | 2 | 3,
  "redundant_steps": ["list any unnecessary steps by stepId"],
  "reasoning": "Explanation"
}
```

## Composite Scoring

Combine multiple dimensions:

```
You are evaluating an AI coding agent's overall performance.

## Task
{input}

## Expected Behavior
{expected}

## Agent Output
{output}

## Full Trajectory
{trajectory}

## Instructions
Evaluate the agent across multiple dimensions.

For each dimension, score 1-3:

1. **Correctness**: Did it produce the right result?
2. **Tool Usage**: Were appropriate tools used efficiently?
3. **Reasoning**: Was the thought process coherent?
4. **Code Quality**: (if applicable) Is the code well-written?

Respond in JSON:
{
  "scores": {
    "correctness": 1 | 2 | 3,
    "tool_usage": 1 | 2 | 3,
    "reasoning": 1 | 2 | 3,
    "code_quality": 1 | 2 | 3 | null
  },
  "overall": 1 | 2 | 3,
  "summary": "One sentence summary of performance",
  "issues": ["list of specific issues found"]
}
```

## Batch Evaluation

For evaluating multiple results at once:

```
You are evaluating a batch of AI agent task completions.

## Evaluation Records
{results_array}

## Instructions
For each record, evaluate:
1. Did the agent complete the task successfully?
2. Was the approach efficient?
3. Were there any notable issues?

Respond as a JSON array with one entry per record:
[
  {
    "id": "record-id",
    "passed": true | false,
    "score": 1 | 2 | 3,
    "issues": ["list of issues"]
  }
]
```

## Template Variables

| Variable | Source | Description |
|----------|--------|-------------|
| `{input}` | `result.input` | Original prompt |
| `{output}` | `result.output` | Final agent response |
| `{expected}` | `result.expected` | Expected output (if provided) |
| `{trajectory}` | `result.trajectory` | Full trajectory array |
| `{step_count}` | `result.trajectory.length` | Number of steps |
| `{tool_call_count}` | Filtered trajectory | Tool call count |
| `{duration}` | `result.timing.end - result.timing.start` | Total duration |
| `{code_from_trajectory}` | Extract from tool inputs | Code written by agent |

## Extracting Code from Trajectory

```typescript
const extractCode = (trajectory: TrajectoryStep[]): string[] => {
  return trajectory
    .filter((s) => s.type === 'tool_call' && s.name === 'Write')
    .map((s) => {
      const input = s.input as { content?: string }
      return input.content ?? ''
    })
    .filter(Boolean)
}
```

## Response Parsing

```typescript
const parseJudgeResponse = (response: string) => {
  // Extract JSON from response (handles markdown code blocks)
  const jsonMatch = response.match(/```json\n?([\s\S]*?)\n?```/) ??
                    response.match(/\{[\s\S]*\}/)

  if (!jsonMatch) {
    throw new Error('No JSON found in judge response')
  }

  const json = jsonMatch[1] ?? jsonMatch[0]
  return JSON.parse(json)
}
```
